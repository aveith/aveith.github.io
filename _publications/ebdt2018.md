---
title: "[Apache Spark](https://link.springer.com/referenceworkentry/10.1007/978-3-319-77525-8_37)"
collection: publications
category: chapters
permalink: /publications/ebdt2018
venue: "EBDT 2018"
date: 2018-6-12
paperurl: 'https://link.springer.com/referenceworkentry/10.1007/978-3-319-77525-8_37'
slidesurl: 
citation: '<b>da Silva Veith, Alexandre</b>; Dias de Assunção, Marcos'
---
[[Paper]](http://aveith.github.io/files/ebdt2018.pdf) [[BIBTEX]](http://aveith.github.io/files/ebdt2018.bib)



## Abstract
Apache Spark is a cluster computing solution and in-memory processing framework that extends the MapReduce model to support other types of computations such as interactive queries and stream processing. Designed to cover a variety of workloads, Spark introduces an abstraction called Resilient Distributed Datasets (RDD) that enables running computations in memory in a fault-tolerant manner. RDDs, which are immutable and partitioned collections of records, provide a programming interface for performing operations, such as map, filter, and join, over multiple data items. For fault-tolerance purposes, Spark records all transformations carried out to build a dataset, thus forming a lineage graph.


